{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.2146  0.5389 -2.2759 -0.1880 -0.3816\n",
       "-1.3382 -0.1660 -0.2584  0.5645  0.3696\n",
       "-0.1445 -2.2646  0.1299  1.2714  1.2196\n",
       "-1.1038 -0.7197  1.0979 -0.6376 -0.6127\n",
       " 0.0215 -0.7507  0.5784  0.9954 -2.5455\n",
       "[torch.cuda.FloatTensor of size 5x5 (GPU 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5,5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Number CUDA Devices: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"__Number CUDA Devices:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string, sys\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../tmp/bandit-nmt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = torch.load('../tmp/bandit-nmt/data/en-zh/processed_all-train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_data = lib.Dataset(dataset[\"valid\"], 64, [0], eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics[\"nmt_loss\"] = lib.Loss.weighted_xent_loss\n",
    "metrics[\"critic_loss\"] = lib.Loss.weighted_mse\n",
    "metrics[\"sent_reward\"] = lib.Reward.sentence_bleu\n",
    "metrics[\"corp_reward\"] = lib.Reward.corpus_bleu\n",
    "dicts = dataset[\"dicts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanigyasi/tmp/anaconda3/lib/python3.6/site-packages/torch/serialization.py:286: SourceChangeWarning: source code of class 'lib.model.EncoderDecoder.Encoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('../tmp/bandit-nmt/scripts/log/model_15_pretrain.pt')\n",
    "model = checkpoint[\"model\"]\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.cuda(0)\n",
    "optim = checkpoint[\"optim\"]\n",
    "start_epoch = checkpoint[\"epoch\"] + 1\n",
    "critic = checkpoint[\"critic\"]\n",
    "critic_optim = checkpoint[\"critic_optim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "total_words = 0\n",
    "total_sents = 0\n",
    "total_sent_reward = 0\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _convert_and_report(data, pred_file, preds, metrics):\n",
    "    preds = data.restore_pos(preds)\n",
    "    with open(pred_file, \"w\") as f:\n",
    "        for sent in preds:\n",
    "            sent = lib.Reward.clean_up_sentence(sent, remove_unk=False, remove_eos=True)\n",
    "            sent = [dicts[\"tgt\"].getLabel(w) for w in sent]\n",
    "            print(\" \".join(sent), file=f)\n",
    "    loss, sent_reward, corpus_reward = metrics\n",
    "    print(\"\")\n",
    "    print(\"Loss: %.6f\" % loss)\n",
    "    print(\"Sentence reward: %.2f\" % (sent_reward * 100))\n",
    "    print(\"Corpus reward: %.2f\" % (corpus_reward * 100))\n",
    "    print(\"Predictions saved to %s\" % pred_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../tmp/bandit-nmt/lib/model/EncoderDecoder.py:24: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  outputs, hidden_t = self.rnn(emb, hidden)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 3.312458\n",
      "Sentence reward: 24.36\n",
      "Corpus reward: 14.46\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(valid_data)):\n",
    "    batch = valid_data[i]\n",
    "    targets = batch[1]\n",
    "    model.decoder.attn.applyMask(batch[0][0].data.eq(lib.Constants.PAD).t())\n",
    "    outputs = model(batch, True)\n",
    "    weights = targets.ne(lib.Constants.PAD).float()\n",
    "    num_words = weights.data.sum()\n",
    "    _, loss = model.predict(outputs, targets, weights, lib.Loss.weighted_xent_loss)\n",
    "    \n",
    "    preds = model.translate(batch, 50)\n",
    "    preds = preds.t().tolist()\n",
    "    targets = targets.data.t().tolist()\n",
    "    rewards, _ = lib.Reward.sentence_bleu(preds, targets)\n",
    "    \n",
    "    all_preds.extend(preds)\n",
    "    all_targets.extend(targets)\n",
    "\n",
    "    total_loss += loss\n",
    "    total_words += num_words\n",
    "    total_sent_reward += sum(rewards)\n",
    "    total_sents += batch[1].size(1)\n",
    "\n",
    "loss = total_loss / total_words\n",
    "sent_reward = total_sent_reward / total_sents\n",
    "corpus_reward = lib.Reward.corpus_bleu(all_preds, all_targets)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Loss: %.6f\" % loss)\n",
    "print(\"Sentence reward: %.2f\" % (sent_reward * 100))\n",
    "print(\"Corpus reward: %.2f\" % (corpus_reward * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', '.']\n"
     ]
    }
   ],
   "source": [
    "preds = valid_data.restore_pos(all_preds)\n",
    "\n",
    "for sent in preds:\n",
    "    sent = lib.Reward.clean_up_sentence(sent, remove_unk=False, remove_eos=True)\n",
    "    sent = [dicts[\"tgt\"].getLabel(w) for w in sent]\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments_seen = {}\n",
    "def evaluate(result, correct):\n",
    "    result = result.split()\n",
    "    print('Result:', result)\n",
    "    if correct in segments_seen:\n",
    "        correct = segments_seen[correct]\n",
    "    else:\n",
    "        segments_seen[correct] = \\\n",
    "        correct.split()\n",
    "        correct = segments_seen[correct]\n",
    "    print('Correct:', correct)\n",
    "    score = lib.Bleu.score_sentence(correct, result, 1)\n",
    "    print('Score:', score[-1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: ['hello', 'my', 'name', 'is', 'crew']\n",
      "Correct: ['hello', 'my', 'name', 'is', 'crew']\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = evaluate('hello my name is crew', 'hello my name is crew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src, tgt, pred = [], [], []\n",
    "sizes = []\n",
    "count, ignored = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: ['它', '可以', '伸展', '到', '150', '英尺', '长', '。']\n",
      "Target: ['it', 'gets', 'up', 'to', 'about', '150', 'feet', 'long', '.']\n",
      "Prediction: ['you', 'see', 'a', 'team', 'of', 'teams', ',', 'students', 'in', 'drama', ',', 'school', 'students', ',', 'learning', 'english', ',', 'a', 'often', 'often', 'list', 'of', 'time', 'for', 'any', 'party', 'and', 'a', 'often', 'often', 'space', '.']\n",
      "Score: 0.00862767574483065\n"
     ]
    }
   ],
   "source": [
    "src_valid = open('../tmp/bandit-nmt/data/en-zh/prep/valid.en-zh.zh')\n",
    "tgt_valid = open('../tmp/bandit-nmt/data/en-zh/prep/valid.en-zh.en')\n",
    "pred_valid = open('../tmp/bandit-nmt/scripts/log/model_15_pretrain.valid.pred')\n",
    "\n",
    "while True:\n",
    "    srcWords = src_valid.readline().split()\n",
    "    tgtWords = tgt_valid.readline().split()\n",
    "    predWords = pred_valid.readline().split()\n",
    "    if not srcWords or not tgtWords:\n",
    "        if srcWords and not tgtWords or not srcWords and tgtWords:\n",
    "            print(\"WARNING: source and target do not have the same number of sentences\")\n",
    "        break\n",
    "\n",
    "    # Only remove long sentences for training set.\n",
    "    if len(srcWords) <= 50 and len(tgtWords) <= 50:\n",
    "        src += [srcWords]\n",
    "        tgt += [tgtWords]\n",
    "        pred += [predWords]\n",
    "        sizes += [len(srcWords)]\n",
    "    else:\n",
    "        ignored += 1\n",
    "\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(\"... %d sentences prepared\" % count)\n",
    "\n",
    "src_valid.close()\n",
    "tgt_valid.close()\n",
    "pred_valid.close()\n",
    "\n",
    "score = lib.Bleu.score_sentence(pred[0], tgt[0],  1)\n",
    "print('Source:', src[0])\n",
    "print('Target:', tgt[0])\n",
    "print('Prediction:', pred[0])\n",
    "print('Score:', score[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
